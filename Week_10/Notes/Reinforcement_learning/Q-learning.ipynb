{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating your First Agent using Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment NChain doesn't exist. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/karina/GitHub/LighthouseLabs2022/Week_10/Notes/Reinforcement_learning/Q-learning.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karina/GitHub/LighthouseLabs2022/Week_10/Notes/Reinforcement_learning/Q-learning.ipynb#ch0000009?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense, InputLayer\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/karina/GitHub/LighthouseLabs2022/Week_10/Notes/Reinforcement_learning/Q-learning.ipynb#ch0000009?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpylab\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/karina/GitHub/LighthouseLabs2022/Week_10/Notes/Reinforcement_learning/Q-learning.ipynb#ch0000009?line=6'>7</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m'\u001b[39;49m\u001b[39mNChain-v0\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py:607\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=600'>601</a>\u001b[0m         logger\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=601'>602</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing the latest versioned environment `\u001b[39m\u001b[39m{\u001b[39;00mnew_env_id\u001b[39m}\u001b[39;00m\u001b[39m` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=602'>603</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minstead of the unversioned environment `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=603'>604</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=605'>606</a>\u001b[0m     \u001b[39mif\u001b[39;00m spec_ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=606'>607</a>\u001b[0m         _check_version_exists(ns, name, version)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=607'>608</a>\u001b[0m         \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo registered env with id: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=609'>610</a>\u001b[0m _kwargs \u001b[39m=\u001b[39m spec_\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py:234\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=230'>231</a>\u001b[0m \u001b[39mif\u001b[39;00m get_env_id(ns, name, version) \u001b[39min\u001b[39;00m registry:\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=231'>232</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=233'>234</a>\u001b[0m _check_name_exists(ns, name)\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=234'>235</a>\u001b[0m \u001b[39mif\u001b[39;00m version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=235'>236</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py:212\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=208'>209</a>\u001b[0m namespace_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m in namespace \u001b[39m\u001b[39m{\u001b[39;00mns\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m ns \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=209'>210</a>\u001b[0m suggestion_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDid you mean: `\u001b[39m\u001b[39m{\u001b[39;00msuggestion[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m`?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m suggestion \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=211'>212</a>\u001b[0m \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mNameNotFound(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=212'>213</a>\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEnvironment \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m doesn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt exist\u001b[39m\u001b[39m{\u001b[39;00mnamespace_msg\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m{\u001b[39;00msuggestion_msg\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/base_env/lib/python3.8/site-packages/gym/envs/registration.py?line=213'>214</a>\u001b[0m )\n",
      "\u001b[0;31mNameNotFound\u001b[0m: Environment NChain doesn't exist. "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "env = gym.make('NChain-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a good policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_sum_reward_agent(env, num_episodes=500):\n",
    "    # this is the table that will hold our summated rewards for\n",
    "    # each action in each state\n",
    "    r_table = np.zeros((5, 2))\n",
    "    for g in range(num_episodes):\n",
    "        s = env.reset() # starts the game afresh each time a new episode is commenced, stores starting state\n",
    "        done = False \n",
    "        while not done: # continues until done signal is passed\n",
    "            if np.sum(r_table[s, :]) == 0: # checks to see if there are any existing values in the r_table for the current state\n",
    "                # make a random selection of actions\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                # select the action with highest cummulative reward\n",
    "                a = np.argmax(r_table[s, :])\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            r_table[s, a] += r\n",
    "            s = new_s # store action the largest row state s.\n",
    "    return r_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delayed Reward Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_with_table(env, num_episodes=500):\n",
    "    q_table = np.zeros((5, 2))\n",
    "    y = 0.95 # gamma\n",
    "    lr = 0.8\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.sum(q_table[s,:]) == 0:\n",
    "                # make a random selection of actions\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                # select the action with largest q value in state s\n",
    "                a = np.argmax(q_table[s, :])\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])\n",
    "            s = new_s\n",
    "    return q_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning with Epsilon - Greedy Action Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_q_learning_with_table(env, num_episodes=500):\n",
    "    q_table = np.zeros((5, 2))\n",
    "    y = 0.95 # gamma\n",
    "    eps = 0.5\n",
    "    lr = 0.8\n",
    "    decay_factor = 0.999\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        eps *= decay_factor\n",
    "        done = False\n",
    "        while not done:\n",
    "            # select the action with highest cummulative reward\n",
    "            if np.random.random() < eps or np.sum(q_table[s, :]) == 0:\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                a = np.argmax(q_table[s, :])\n",
    "            # pdb.set_trace()\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            q_table[s, a] += r + lr * (y * np.max(q_table[new_s, :]) - q_table[s, a])\n",
    "            s = new_s\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(table, env):\n",
    "    s = env.reset()\n",
    "    tot_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = np.argmax(table[s, :])\n",
    "        s, r, done, _ = env.step(a)\n",
    "        tot_reward += r\n",
    "    return tot_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_methods(env, num_iterations=100):\n",
    "    winner = np.zeros((3,))\n",
    "    for g in range(num_iterations):\n",
    "        m0_table = naive_sum_reward_agent(env, 500)\n",
    "        m1_table = q_learning_with_table(env, 500)\n",
    "        m2_table = eps_greedy_q_learning_with_table(env, 500)\n",
    "        m0 = run_game(m0_table, env)\n",
    "        m1 = run_game(m1_table, env)\n",
    "        m2 = run_game(m2_table, env)\n",
    "        w = np.argmax(np.array([m0, m1, m2]))\n",
    "        winner[w] += 1\n",
    "        print(\"Game {} of {}\".format(g + 1, num_iterations))\n",
    "    return winner"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39512f3c2a1741d7f752d45a133d4514127029333ea14bc2f3c6c5e6759b9029"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
