{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/13 21:07:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "session = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Interact with RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Resilient Distributed Dataset (RDD)\n",
    "rdd = session.sparkContext.parallelize([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return first 2 values\n",
    "rdd.take(num=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return the length of RDD\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Send all RDD data to the driver as an array\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe\n",
    "df = session.createDataFrame(\n",
    "  [[1,2,3], [4,5,6]], ['column1', 'column2', 'column3']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|column1|column2|column3|\n",
      "+-------+-------+-------+\n",
      "|      1|      2|      3|\n",
      "|      4|      5|      6|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display dataframe with first n rows\n",
    "df.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe manipulation - User Defined Functions (UDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+----------+\n",
      "|column1|column2|column3|multiplied|\n",
      "+-------+-------+-------+----------+\n",
      "|      1|      2|      3|      10.0|\n",
      "|      4|      5|      6|      40.0|\n",
      "+-------+-------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as funcs\n",
    "import pyspark.sql.types as types\n",
    "def multiply_by_ten(number):\n",
    "    return number*10.0\n",
    "multiply_udf = funcs.udf(multiply_by_ten, types.DoubleType())\n",
    "transformed_df = df.withColumn(\n",
    "    'multiplied', multiply_udf('column1')\n",
    ")\n",
    "transformed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as types\n",
    "import math\n",
    "def take_log_in_all_columns(row: types.Row):\n",
    "     old_row = row.asDict()\n",
    "     new_row = {f'log({column_name})': math.log(value) \n",
    "                for column_name, value in old_row.items()}\n",
    "     return types.Row(**new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logarithmic_dataframe = df.rdd.map(take_log_in_all_columns).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+\n",
      "|      log(column1)|      log(column2)|      log(column3)|\n",
      "+------------------+------------------+------------------+\n",
      "|               0.0|0.6931471805599453|1.0986122886681098|\n",
      "|1.3862943611198906|1.6094379124341003| 1.791759469228055|\n",
      "+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logarithmic_dataframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[column1: bigint, column2: bigint]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SElECT\n",
    "df.select('column1', 'column2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[column1: bigint, column2: bigint, column3: bigint]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WHERE\n",
    "df.where('column1 = 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INNER JOIN\n",
    "df.join(df1, ['column1'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal view of dataframe\n",
    "df.createOrReplaceTempView(“table1”)\n",
    "\n",
    "# Perform query on top of view\n",
    "df2 = session.sql(\"SELECT column1 AS f1, column2 as f2 from table1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Column Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.withColumn(\n",
    "    'derived_column', df['column1'] + df['column2'] * df['column3']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations and quick Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADULT_COLUMN_NAMES = [\n",
    "     \"age\",\n",
    "     \"workclass\",\n",
    "     \"fnlwgt\",\n",
    "     \"education\",\n",
    "     \"education_num\",\n",
    "     \"marital_status\",\n",
    "     \"occupation\",\n",
    "     \"relationship\",\n",
    "     \"race\",\n",
    "     \"sex\",\n",
    "     \"capital_gain\",\n",
    "     \"capital_loss\",\n",
    "     \"hours_per_week\",\n",
    "     \"native_country\",\n",
    "     \"income\"\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df = session.read.csv(\n",
    "     'data/adult.data.csv', header=False, inferSchema=True\n",
    " )\n",
    "for new_col, old_col in zip(ADULT_COLUMN_NAMES, csv_df.columns):\n",
    "     csv_df = csv_df.withColumnRenamed(old_col, new_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_hours_df = csv_df.groupBy(\n",
    "    'age'\n",
    ").agg(\n",
    "    funcs.avg('hours_per_week'),\n",
    "    funcs.stddev_samp('hours_per_week')\n",
    ").sort('age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/13 21:23:22 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "22/07/13 22:14:10 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 399268 ms exceeds timeout 120000 ms\n",
      "22/07/13 22:14:10 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "session = SparkSession.builder.config(\n",
    "    'spark.jars', 'bin/postgresql-42.2.16.jar'\n",
    ").config(\n",
    "    'spark.driver.extraClassPath', 'bin/postgresql-42.2.16.jar'\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"jdbc:postgresql://your_host_ip:5432/your_database\"\n",
    "properties = {'user': 'your_user', 'password': 'your_password'}\n",
    "# read from a table into a dataframe\n",
    "df = session.read.jdbc(\n",
    "    url=url, table='your_table_name', properties=properties\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.write.jdbc(\n",
    "    url=url, table='new_table', mode='append', properties=properties\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39512f3c2a1741d7f752d45a133d4514127029333ea14bc2f3c6c5e6759b9029"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
